{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Transformer Architecture\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [correct `transformer implementation` from scratch in `pytorch`](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb) (in-depth tutorial from same author: [part1](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021), [part2](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada))\n",
    "- [nice visuals for understanding `multi-head attention`](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [`positional encoding`](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)\n",
    "- [kaggle transformer code: ❗Contains mistakes (see comments), but nice overall explanation](https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "For details about the dataset see `data_handling.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_dominant_hand_0</th>\n",
       "      <th>x_dominant_hand_1</th>\n",
       "      <th>x_dominant_hand_2</th>\n",
       "      <th>x_dominant_hand_3</th>\n",
       "      <th>x_dominant_hand_4</th>\n",
       "      <th>x_dominant_hand_5</th>\n",
       "      <th>x_dominant_hand_6</th>\n",
       "      <th>x_dominant_hand_7</th>\n",
       "      <th>x_dominant_hand_8</th>\n",
       "      <th>x_dominant_hand_9</th>\n",
       "      <th>...</th>\n",
       "      <th>z_dominant_hand_11</th>\n",
       "      <th>z_dominant_hand_12</th>\n",
       "      <th>z_dominant_hand_13</th>\n",
       "      <th>z_dominant_hand_14</th>\n",
       "      <th>z_dominant_hand_15</th>\n",
       "      <th>z_dominant_hand_16</th>\n",
       "      <th>z_dominant_hand_17</th>\n",
       "      <th>z_dominant_hand_18</th>\n",
       "      <th>z_dominant_hand_19</th>\n",
       "      <th>z_dominant_hand_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1816796431</th>\n",
       "      <td>0.408832</td>\n",
       "      <td>0.519912</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.707576</td>\n",
       "      <td>0.797313</td>\n",
       "      <td>0.494709</td>\n",
       "      <td>0.532817</td>\n",
       "      <td>0.553556</td>\n",
       "      <td>0.566219</td>\n",
       "      <td>0.391196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245855</td>\n",
       "      <td>-0.269148</td>\n",
       "      <td>-0.129743</td>\n",
       "      <td>-0.251501</td>\n",
       "      <td>-0.278687</td>\n",
       "      <td>-0.266530</td>\n",
       "      <td>-0.152852</td>\n",
       "      <td>-0.257519</td>\n",
       "      <td>-0.275822</td>\n",
       "      <td>-0.266876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816796431</th>\n",
       "      <td>0.398663</td>\n",
       "      <td>0.523662</td>\n",
       "      <td>0.638807</td>\n",
       "      <td>0.744236</td>\n",
       "      <td>0.832567</td>\n",
       "      <td>0.538486</td>\n",
       "      <td>0.564302</td>\n",
       "      <td>0.581011</td>\n",
       "      <td>0.597674</td>\n",
       "      <td>0.441541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.370770</td>\n",
       "      <td>-0.408097</td>\n",
       "      <td>-0.185217</td>\n",
       "      <td>-0.325494</td>\n",
       "      <td>-0.343373</td>\n",
       "      <td>-0.328294</td>\n",
       "      <td>-0.203126</td>\n",
       "      <td>-0.315719</td>\n",
       "      <td>-0.326104</td>\n",
       "      <td>-0.314282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816796431</th>\n",
       "      <td>0.419290</td>\n",
       "      <td>0.509726</td>\n",
       "      <td>0.593165</td>\n",
       "      <td>0.685492</td>\n",
       "      <td>0.777913</td>\n",
       "      <td>0.483669</td>\n",
       "      <td>0.510993</td>\n",
       "      <td>0.536410</td>\n",
       "      <td>0.564583</td>\n",
       "      <td>0.393016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285770</td>\n",
       "      <td>-0.318548</td>\n",
       "      <td>-0.155317</td>\n",
       "      <td>-0.274822</td>\n",
       "      <td>-0.312119</td>\n",
       "      <td>-0.316411</td>\n",
       "      <td>-0.181363</td>\n",
       "      <td>-0.286298</td>\n",
       "      <td>-0.316182</td>\n",
       "      <td>-0.322671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816796431</th>\n",
       "      <td>0.398764</td>\n",
       "      <td>0.498118</td>\n",
       "      <td>0.583356</td>\n",
       "      <td>0.677779</td>\n",
       "      <td>0.775966</td>\n",
       "      <td>0.481279</td>\n",
       "      <td>0.491659</td>\n",
       "      <td>0.524974</td>\n",
       "      <td>0.571944</td>\n",
       "      <td>0.412262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235725</td>\n",
       "      <td>-0.267054</td>\n",
       "      <td>-0.141380</td>\n",
       "      <td>-0.219369</td>\n",
       "      <td>-0.256553</td>\n",
       "      <td>-0.273690</td>\n",
       "      <td>-0.170996</td>\n",
       "      <td>-0.240285</td>\n",
       "      <td>-0.266193</td>\n",
       "      <td>-0.278110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816796431</th>\n",
       "      <td>0.420213</td>\n",
       "      <td>0.495650</td>\n",
       "      <td>0.571790</td>\n",
       "      <td>0.659049</td>\n",
       "      <td>0.749740</td>\n",
       "      <td>0.485707</td>\n",
       "      <td>0.475930</td>\n",
       "      <td>0.501727</td>\n",
       "      <td>0.539150</td>\n",
       "      <td>0.438294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186706</td>\n",
       "      <td>-0.217181</td>\n",
       "      <td>-0.107740</td>\n",
       "      <td>-0.165642</td>\n",
       "      <td>-0.201059</td>\n",
       "      <td>-0.222898</td>\n",
       "      <td>-0.131329</td>\n",
       "      <td>-0.183113</td>\n",
       "      <td>-0.208774</td>\n",
       "      <td>-0.225284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x_dominant_hand_0  x_dominant_hand_1  x_dominant_hand_2  \\\n",
       "sequence_id                                                            \n",
       "1816796431            0.408832           0.519912           0.612159   \n",
       "1816796431            0.398663           0.523662           0.638807   \n",
       "1816796431            0.419290           0.509726           0.593165   \n",
       "1816796431            0.398764           0.498118           0.583356   \n",
       "1816796431            0.420213           0.495650           0.571790   \n",
       "\n",
       "             x_dominant_hand_3  x_dominant_hand_4  x_dominant_hand_5  \\\n",
       "sequence_id                                                            \n",
       "1816796431            0.707576           0.797313           0.494709   \n",
       "1816796431            0.744236           0.832567           0.538486   \n",
       "1816796431            0.685492           0.777913           0.483669   \n",
       "1816796431            0.677779           0.775966           0.481279   \n",
       "1816796431            0.659049           0.749740           0.485707   \n",
       "\n",
       "             x_dominant_hand_6  x_dominant_hand_7  x_dominant_hand_8  \\\n",
       "sequence_id                                                            \n",
       "1816796431            0.532817           0.553556           0.566219   \n",
       "1816796431            0.564302           0.581011           0.597674   \n",
       "1816796431            0.510993           0.536410           0.564583   \n",
       "1816796431            0.491659           0.524974           0.571944   \n",
       "1816796431            0.475930           0.501727           0.539150   \n",
       "\n",
       "             x_dominant_hand_9  ...  z_dominant_hand_11  z_dominant_hand_12  \\\n",
       "sequence_id                     ...                                           \n",
       "1816796431            0.391196  ...           -0.245855           -0.269148   \n",
       "1816796431            0.441541  ...           -0.370770           -0.408097   \n",
       "1816796431            0.393016  ...           -0.285770           -0.318548   \n",
       "1816796431            0.412262  ...           -0.235725           -0.267054   \n",
       "1816796431            0.438294  ...           -0.186706           -0.217181   \n",
       "\n",
       "             z_dominant_hand_13  z_dominant_hand_14  z_dominant_hand_15  \\\n",
       "sequence_id                                                               \n",
       "1816796431            -0.129743           -0.251501           -0.278687   \n",
       "1816796431            -0.185217           -0.325494           -0.343373   \n",
       "1816796431            -0.155317           -0.274822           -0.312119   \n",
       "1816796431            -0.141380           -0.219369           -0.256553   \n",
       "1816796431            -0.107740           -0.165642           -0.201059   \n",
       "\n",
       "             z_dominant_hand_16  z_dominant_hand_17  z_dominant_hand_18  \\\n",
       "sequence_id                                                               \n",
       "1816796431            -0.266530           -0.152852           -0.257519   \n",
       "1816796431            -0.328294           -0.203126           -0.315719   \n",
       "1816796431            -0.316411           -0.181363           -0.286298   \n",
       "1816796431            -0.273690           -0.170996           -0.240285   \n",
       "1816796431            -0.222898           -0.131329           -0.183113   \n",
       "\n",
       "             z_dominant_hand_19  z_dominant_hand_20  \n",
       "sequence_id                                          \n",
       "1816796431            -0.275822           -0.266876  \n",
       "1816796431            -0.326104           -0.314282  \n",
       "1816796431            -0.316182           -0.322671  \n",
       "1816796431            -0.266193           -0.278110  \n",
       "1816796431            -0.208774           -0.225284  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_FOLDER = \"processed_dataset\"\n",
    "\n",
    "df = pd.read_pickle(os.path.join(SAVE_FOLDER, \"data.pkl\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{1816796431: {'phrase': array([ 1,  7,  3, 16, 31, 18, 18, 24, 21, 28, 34, 32, 18,  2,  0,  0,  0,\\n ...\""
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(os.path.join(SAVE_FOLDER, \"metadata.csv\"), header=0)\n",
    "\n",
    "max_phrase_len = max([len(it) for it in metadata_df.phrase.values])\n",
    "possible_characters = sorted(set.union(*[set(p) for p in metadata_df.phrase.values]))\n",
    "token_map = {c: i+3 for i, c in enumerate(possible_characters)}\n",
    "token_map['P'] = 0 # padding\n",
    "token_map['<'] = 1 # SOS\n",
    "token_map['>'] = 2 # EOS\n",
    "metadata_df.phrase = metadata_df.phrase.apply(lambda it: np.array([token_map[c] for c in '<'+it+'>'+('P'*(max_phrase_len-len(it)))], dtype=np.int32))\n",
    "\n",
    "index = {row[0]: {\"phrase\": row[1], \"signer_id\": row[2]} for row in metadata_df.to_numpy()}\n",
    "str(index)[:100] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sequence_ids = df.index.unique()\n",
    "\n",
    "# train_ids, temp_ids = train_test_split(sequence_ids, test_size=0.3, random_state=seed)\n",
    "# valid_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=seed)\n",
    "\n",
    "# train_df = df[df.index.isin(train_ids)]\n",
    "# valid_df = df[df.index.isin(valid_ids)]\n",
    "# test_df = df[df.index.isin(test_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution-based Transformer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, meta_data, seq_len=256, padding_value=0.0):\n",
    "        self.df = df\n",
    "        self.meta_data = meta_data\n",
    "        self.sequence_ids = df.index.unique()\n",
    "        self.padding_value = padding_value\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_id = self.sequence_ids[idx]\n",
    "        x_values = torch.tensor(self.df.loc[sequence_id].values, dtype=torch.float32)\n",
    "\n",
    "        x_mask = torch.ones(self.seq_len, dtype=torch.float32)\n",
    "\n",
    "        # Apply padding if the sequence is shorter than seq_len\n",
    "        if x_values.shape[0] < self.seq_len:\n",
    "            x_mask[x_values.shape[0]:] = 0.0\n",
    "            padding_size = self.seq_len - x_values.shape[0]\n",
    "            padding = torch.full((padding_size, x_values.shape[1]), self.padding_value)\n",
    "            x_values = torch.cat([x_values, padding], dim=0)\n",
    "        elif x_values.shape[0] > self.seq_len:\n",
    "            # Truncate the sequence if it's longer than seq_len\n",
    "            x_values = x_values[:self.seq_len]\n",
    "\n",
    "        y_phrase = self.meta_data[sequence_id]['phrase']\n",
    "        return x_values, x_mask, y_phrase\n",
    "\n",
    "dataset = TransformerDataset(df, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4088,  0.5199,  0.6122,  ..., -0.2575, -0.2758, -0.2669],\n",
       "         [ 0.3987,  0.5237,  0.6388,  ..., -0.3157, -0.3261, -0.3143],\n",
       "         [ 0.4193,  0.5097,  0.5932,  ..., -0.2863, -0.3162, -0.3227],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]),\n",
       " array([ 1,  7,  3, 16, 31, 18, 18, 24, 21, 28, 34, 32, 18,  2,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x:\n",
      "torch.Size([32, 256, 63])\n",
      "\n",
      "Mask for input:\n",
      "torch.Size([32, 256])\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "\n",
      "Target:\n",
      "torch.Size([32, 33])\n",
      "tensor([[ 1,  7,  3,  ...,  0,  0,  0],\n",
      "        [ 1,  5,  7,  ...,  0,  0,  0],\n",
      "        [ 1, 13, 12,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 1,  8,  5,  ...,  0,  0,  0],\n",
      "        [ 1,  8,  5,  ...,  0,  0,  0],\n",
      "        [ 1, 26, 14,  ...,  0,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# NOTE Refactor DataSet (move huge objects outside) before increasing number of workers\n",
    "transformer_dataloader = DataLoader(dataset, batch_size=32, num_workers=0)\n",
    "\n",
    "for it in transformer_dataloader:\n",
    "    print(\"Shape of x:\")\n",
    "    print(it[0].shape)\n",
    "    print(\"\\nMask for input:\")\n",
    "    print(it[1].shape)\n",
    "    print(it[1])\n",
    "    print(\"\\nTarget:\")\n",
    "    print(it[2].shape)\n",
    "    print(it[2]) # NOTE dec inp: it[1][:, :-1], target: it[1][:, 1:]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 ms ± 1.96 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit next(iter(transformer_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN-based Transformer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/loader/dataloader.py\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "class SequentialGraphDataLoader:\n",
    "    def __init__(self, dataset: TransformerDataset, batch_size: int = 32, shuffle: bool = False):\n",
    "        # Wrap the TransformerDataset into default data loader\n",
    "        self.transformer_dataloader = DataLoader(\n",
    "            dataset=dataset, batch_size=batch_size, num_workers=0, shuffle=shuffle\n",
    "        )\n",
    "\n",
    "        # Precompute edge_index and \"batch\" for batch\n",
    "        connections = [\n",
    "            (0,1), (0,5), (0,17), (1,2), (2,3), (3,4),\n",
    "            (5,6), (5,9), (6,7), (7,8), (9,10), (9,13),\n",
    "            (10,11), (11,12), (13,14), (13,17), (14,15),\n",
    "            (15,16), (17,18), (18,19), (19,20)\n",
    "        ]\n",
    "        edges = []\n",
    "        for a, b in connections:\n",
    "            edges.append([a, b])\n",
    "            edges.append([b, a])  # Add the reverse connection\n",
    "        self.edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "        # NOTE Edge index calculation can be done without using Batch.from_data_list(...) -> improve performance\n",
    "        # For now this is good enough for a one time run\n",
    "        example_batch = next(iter(self.transformer_dataloader))\n",
    "        seq_len = len(example_batch[0][0])\n",
    "        example_graph_features = torch.zeros(21, 3)\n",
    "        data_list = [Data(x=example_graph_features, edge_index=self.edge_index) for _ in range(batch_size*seq_len)]\n",
    "        self.mini_batch = Batch.from_data_list(data_list)\n",
    "        self.edge_index = self.mini_batch.edge_index\n",
    "        self.batch = self.mini_batch.batch\n",
    "\n",
    "        self.iterator = iter(self.transformer_dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.iterator = iter(self.transformer_dataloader)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        data = next(self.iterator)\n",
    "        return [*data, self.edge_index, self.batch]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.transformer_dataloader) # NOTE how many batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 ms ± 2.77 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "graph_dataloader = SequentialGraphDataLoader(dataset, batch_size=32)\n",
    "\n",
    "# for it in graph_dataloader:\n",
    "#     break\n",
    "\n",
    "%timeit next(iter(graph_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len, n=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        assert d_model % 2 == 0, \"d_model must be even\"\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(n) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # NOTE\n",
    "        # register buffer in Pytorch ->\n",
    "        # If you have parameters in your model, which should be saved and restored in the state_dict,\n",
    "        # but not trained by the optimizer, you should register them as buffers.\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Broadcasting mechanism (automatically works for multiple batches even when shapes don't match along batch dim)\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalLandmarkEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model, num_features, num_conv_layers, filter_size):\n",
    "        super(PositionalLandmarkEmbedding, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        first_conv = nn.Conv1d(in_channels=num_features, out_channels=d_model, kernel_size=filter_size, padding='same')\n",
    "        rest_of_convs = [\n",
    "            nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=filter_size, padding='same')\n",
    "            for _ in range(num_conv_layers-1)\n",
    "        ]\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            first_conv, *rest_of_convs\n",
    "        )\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be of shape (batch_size, seq_len, num_of_features)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, num_of_features, seq_len)\n",
    "        x = self.conv_block(x)\n",
    "        # TODO experiment with scaling the output\n",
    "        # x *= math.sqrt(self.d_model)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, seq_len, d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.9 ms ± 6.14 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "ple = PositionalLandmarkEmbedding(256, 128, 63, 3, 11)\n",
    "x = next(iter(transformer_dataloader))[0]\n",
    "%timeit ple(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "class PositionalGraphEmbedding(nn.Module):\n",
    "    def __init__(self, max_seq_len, d_model, num_features_per_node):\n",
    "        super(PositionalGraphEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_features_per_node = num_features_per_node\n",
    "\n",
    "        # TODO experiment with hyperparameters\n",
    "        hidden_dim = 256\n",
    "        self.conv1 = GATConv(num_features_per_node, hidden_dim, heads=1, concat=False)\n",
    "        self.conv2 = GATConv(hidden_dim, d_model, heads=1, concat=False)\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # x ~ (batch_size, sequence_len, num_features)\n",
    "        batch_size = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        x = x.reshape(-1, self.num_features_per_node) # (batch_size*sequence_len*num_nodes, num_features_per_node)\n",
    "\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # (batch_size*sequence_len, d_model)\n",
    "\n",
    "        x = x.reshape(batch_size, seq_len, self.d_model)\n",
    "        x = self.pos_encoding(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, x_mask, y, edge_index, batch = next(iter(graph_dataloader))\n",
    "layer = PositionalGraphEmbedding(256, 128, 3)\n",
    "\n",
    "# NOTE\n",
    "# Running the Graph-based layers on each graph separately is more than 10x slower than running on the mini batch\n",
    "# To make it faster:\n",
    "# - run on GPU\n",
    "# - rewrite GNN layers using the fact that all graphs will have the same structure\n",
    "%timeit layer(x, edge_index, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest of the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.masked_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.masked_self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_inp_features, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        # self.encoder_embedding = PositionalLandmarkEmbedding(\n",
    "        #     max_seq_len=max_seq_length,\n",
    "        #     d_model=d_model,\n",
    "        #     num_features=num_inp_features,\n",
    "        #     num_conv_layers=3, # TODO experiment with this!\n",
    "        #     filter_size=11 # TODO experiment with this!\n",
    "        # )\n",
    "        self.encoder_embedding = PositionalGraphEmbedding(\n",
    "            max_seq_len=max_seq_length,\n",
    "            d_model=d_model,\n",
    "            num_features_per_node=3\n",
    "        )\n",
    "\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, x_mask, tgt, edge_index=None, batch=None):\n",
    "        src_mask, tgt_mask = self.generate_mask(x_mask, tgt)\n",
    "        src_embedded = self.dropout(self.encoder_embedding(src, edge_index, batch))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_vocab_size=len(token_map)\n",
    "\n",
    "transformer = Transformer(\n",
    "    num_inp_features=63,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=128,\n",
    "    num_heads=8,\n",
    "    num_layers=2,\n",
    "    d_ff=64,\n",
    "    max_seq_length=max(max_phrase_len+2, 256),\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional lr scheduling as in Attention Is All You Need\n",
    "# class LinearWarmupInverseSquarerootDecay(torch.optim.lr_scheduler.LambdaLR):\n",
    "#     def __init__(self, d_model, warmup_steps=4000, optimizer=None):\n",
    "#         self.d_model = d_model\n",
    "#         self.warmup_steps = warmup_steps\n",
    "#         super(LinearWarmupInverseSquarerootDecay, self).__init__(optimizer, self.lr_lambda)\n",
    "\n",
    "#     def lr_lambda(self, step_num):\n",
    "#         return (self.d_model ** -0.5) * min(step_num ** -0.5 if step_num != 0 else 1e20, step_num * (self.warmup_steps ** -1.5))\n",
    "    \n",
    "# steps = np.arange(0, 10000)\n",
    "# learning_rates = [scheduler.lr_lambda(step) for step in steps]\n",
    "\n",
    "# plt.figure(figsize=(6, 3))\n",
    "# plt.plot(steps, learning_rates, label='Learning Rate')\n",
    "# plt.title('Learning Rate Schedule')\n",
    "# plt.xlabel('Step Number')\n",
    "# plt.ylabel('Learning Rate')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# scheduler = LinearWarmupInverseSquarerootDecay(d_model=d_model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0) # ignoring padding\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0)\n",
    "# optimizer = torch.optim.Adam(transformer.parameters())\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(1):\n",
    "    epoch_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch in graph_dataloader:\n",
    "        # src_data, x_mask, tgt_data = batch\n",
    "        src_data, x_mask, tgt_data, edge_index, batch_ = batch\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(src_data, x_mask, tgt_data[:, :-1])\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1).long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        output_tokens = output.argmax(dim=-1)  # Shape: (batch_size, seq_len)\n",
    "        non_pad_mask = tgt_data[:, 1:] != 0  # Ignore padding tokens (mask for target data)\n",
    "        correct = (output_tokens == tgt_data[:, 1:]) & non_pad_mask  # Compare predictions to targets and ignore padding\n",
    "        total_correct += correct.sum().item()\n",
    "        total_tokens += non_pad_mask.sum().item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(graph_dataloader)\n",
    "    accuracy = total_correct / total_tokens if total_tokens > 0 else 0\n",
    "    print(f\"Epoch: {epoch+1}, Average Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# TODO\n",
    "# Masked Accuracy (+other metrics)\n",
    "# Shuffle dataset not only in the batch\n",
    "# read extra stuff in readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
