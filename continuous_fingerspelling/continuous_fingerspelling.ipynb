{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified Transformer Architecture\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [correct `transformer implementation` from scratch in `pytorch`](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb) (in-depth tutorial from same author: [part1](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-1-552f0b41d021), [part2](https://towardsdatascience.com/all-you-need-to-know-about-attention-and-transformers-in-depth-understanding-part-2-bf2403804ada))\n",
    "- [nice visuals for understanding `multi-head attention`](http://jalammar.github.io/illustrated-transformer/)\n",
    "- [`positional encoding`](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)\n",
    "- [kaggle transformer code: ❗Contains mistakes (see comments), but nice overall explanation](https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.13\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.4.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "print(f\"torch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "For details about the dataset see `data_handling.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_dominant_hand_0</th>\n",
       "      <th>x_dominant_hand_1</th>\n",
       "      <th>x_dominant_hand_2</th>\n",
       "      <th>x_dominant_hand_3</th>\n",
       "      <th>x_dominant_hand_4</th>\n",
       "      <th>x_dominant_hand_5</th>\n",
       "      <th>x_dominant_hand_6</th>\n",
       "      <th>x_dominant_hand_7</th>\n",
       "      <th>x_dominant_hand_8</th>\n",
       "      <th>x_dominant_hand_9</th>\n",
       "      <th>...</th>\n",
       "      <th>z_dominant_hand_11</th>\n",
       "      <th>z_dominant_hand_12</th>\n",
       "      <th>z_dominant_hand_13</th>\n",
       "      <th>z_dominant_hand_14</th>\n",
       "      <th>z_dominant_hand_15</th>\n",
       "      <th>z_dominant_hand_16</th>\n",
       "      <th>z_dominant_hand_17</th>\n",
       "      <th>z_dominant_hand_18</th>\n",
       "      <th>z_dominant_hand_19</th>\n",
       "      <th>z_dominant_hand_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1816796431</th>\n",
       "      <td>0.408832</td>\n",
       "      <td>0.519912</td>\n",
       "      <td>0.612159</td>\n",
       "      <td>0.707576</td>\n",
       "      <td>0.797313</td>\n",
       "      <td>0.494709</td>\n",
       "      <td>0.532817</td>\n",
       "      <td>0.553556</td>\n",
       "      <td>0.566219</td>\n",
       "      <td>0.391196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245855</td>\n",
       "      <td>-0.269148</td>\n",
       "      <td>-0.129743</td>\n",
       "      <td>-0.251501</td>\n",
       "      <td>-0.278687</td>\n",
       "      <td>-0.266530</td>\n",
       "      <td>-0.152852</td>\n",
       "      <td>-0.257519</td>\n",
       "      <td>-0.275822</td>\n",
       "      <td>-0.266876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816796431</th>\n",
       "      <td>0.398663</td>\n",
       "      <td>0.523662</td>\n",
       "      <td>0.638807</td>\n",
       "      <td>0.744236</td>\n",
       "      <td>0.832567</td>\n",
       "      <td>0.538486</td>\n",
       "      <td>0.564302</td>\n",
       "      <td>0.581011</td>\n",
       "      <td>0.597674</td>\n",
       "      <td>0.441541</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.370770</td>\n",
       "      <td>-0.408097</td>\n",
       "      <td>-0.185217</td>\n",
       "      <td>-0.325494</td>\n",
       "      <td>-0.343373</td>\n",
       "      <td>-0.328294</td>\n",
       "      <td>-0.203126</td>\n",
       "      <td>-0.315719</td>\n",
       "      <td>-0.326104</td>\n",
       "      <td>-0.314282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816796431</th>\n",
       "      <td>0.419290</td>\n",
       "      <td>0.509726</td>\n",
       "      <td>0.593165</td>\n",
       "      <td>0.685492</td>\n",
       "      <td>0.777913</td>\n",
       "      <td>0.483669</td>\n",
       "      <td>0.510993</td>\n",
       "      <td>0.536410</td>\n",
       "      <td>0.564583</td>\n",
       "      <td>0.393016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285770</td>\n",
       "      <td>-0.318548</td>\n",
       "      <td>-0.155317</td>\n",
       "      <td>-0.274822</td>\n",
       "      <td>-0.312119</td>\n",
       "      <td>-0.316411</td>\n",
       "      <td>-0.181363</td>\n",
       "      <td>-0.286298</td>\n",
       "      <td>-0.316182</td>\n",
       "      <td>-0.322671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816796431</th>\n",
       "      <td>0.398764</td>\n",
       "      <td>0.498118</td>\n",
       "      <td>0.583356</td>\n",
       "      <td>0.677779</td>\n",
       "      <td>0.775966</td>\n",
       "      <td>0.481279</td>\n",
       "      <td>0.491659</td>\n",
       "      <td>0.524974</td>\n",
       "      <td>0.571944</td>\n",
       "      <td>0.412262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.235725</td>\n",
       "      <td>-0.267054</td>\n",
       "      <td>-0.141380</td>\n",
       "      <td>-0.219369</td>\n",
       "      <td>-0.256553</td>\n",
       "      <td>-0.273690</td>\n",
       "      <td>-0.170996</td>\n",
       "      <td>-0.240285</td>\n",
       "      <td>-0.266193</td>\n",
       "      <td>-0.278110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816796431</th>\n",
       "      <td>0.420213</td>\n",
       "      <td>0.495650</td>\n",
       "      <td>0.571790</td>\n",
       "      <td>0.659049</td>\n",
       "      <td>0.749740</td>\n",
       "      <td>0.485707</td>\n",
       "      <td>0.475930</td>\n",
       "      <td>0.501727</td>\n",
       "      <td>0.539150</td>\n",
       "      <td>0.438294</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186706</td>\n",
       "      <td>-0.217181</td>\n",
       "      <td>-0.107740</td>\n",
       "      <td>-0.165642</td>\n",
       "      <td>-0.201059</td>\n",
       "      <td>-0.222898</td>\n",
       "      <td>-0.131329</td>\n",
       "      <td>-0.183113</td>\n",
       "      <td>-0.208774</td>\n",
       "      <td>-0.225284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x_dominant_hand_0  x_dominant_hand_1  x_dominant_hand_2  \\\n",
       "sequence_id                                                            \n",
       "1816796431            0.408832           0.519912           0.612159   \n",
       "1816796431            0.398663           0.523662           0.638807   \n",
       "1816796431            0.419290           0.509726           0.593165   \n",
       "1816796431            0.398764           0.498118           0.583356   \n",
       "1816796431            0.420213           0.495650           0.571790   \n",
       "\n",
       "             x_dominant_hand_3  x_dominant_hand_4  x_dominant_hand_5  \\\n",
       "sequence_id                                                            \n",
       "1816796431            0.707576           0.797313           0.494709   \n",
       "1816796431            0.744236           0.832567           0.538486   \n",
       "1816796431            0.685492           0.777913           0.483669   \n",
       "1816796431            0.677779           0.775966           0.481279   \n",
       "1816796431            0.659049           0.749740           0.485707   \n",
       "\n",
       "             x_dominant_hand_6  x_dominant_hand_7  x_dominant_hand_8  \\\n",
       "sequence_id                                                            \n",
       "1816796431            0.532817           0.553556           0.566219   \n",
       "1816796431            0.564302           0.581011           0.597674   \n",
       "1816796431            0.510993           0.536410           0.564583   \n",
       "1816796431            0.491659           0.524974           0.571944   \n",
       "1816796431            0.475930           0.501727           0.539150   \n",
       "\n",
       "             x_dominant_hand_9  ...  z_dominant_hand_11  z_dominant_hand_12  \\\n",
       "sequence_id                     ...                                           \n",
       "1816796431            0.391196  ...           -0.245855           -0.269148   \n",
       "1816796431            0.441541  ...           -0.370770           -0.408097   \n",
       "1816796431            0.393016  ...           -0.285770           -0.318548   \n",
       "1816796431            0.412262  ...           -0.235725           -0.267054   \n",
       "1816796431            0.438294  ...           -0.186706           -0.217181   \n",
       "\n",
       "             z_dominant_hand_13  z_dominant_hand_14  z_dominant_hand_15  \\\n",
       "sequence_id                                                               \n",
       "1816796431            -0.129743           -0.251501           -0.278687   \n",
       "1816796431            -0.185217           -0.325494           -0.343373   \n",
       "1816796431            -0.155317           -0.274822           -0.312119   \n",
       "1816796431            -0.141380           -0.219369           -0.256553   \n",
       "1816796431            -0.107740           -0.165642           -0.201059   \n",
       "\n",
       "             z_dominant_hand_16  z_dominant_hand_17  z_dominant_hand_18  \\\n",
       "sequence_id                                                               \n",
       "1816796431            -0.266530           -0.152852           -0.257519   \n",
       "1816796431            -0.328294           -0.203126           -0.315719   \n",
       "1816796431            -0.316411           -0.181363           -0.286298   \n",
       "1816796431            -0.273690           -0.170996           -0.240285   \n",
       "1816796431            -0.222898           -0.131329           -0.183113   \n",
       "\n",
       "             z_dominant_hand_19  z_dominant_hand_20  \n",
       "sequence_id                                          \n",
       "1816796431            -0.275822           -0.266876  \n",
       "1816796431            -0.326104           -0.314282  \n",
       "1816796431            -0.316182           -0.322671  \n",
       "1816796431            -0.266193           -0.278110  \n",
       "1816796431            -0.208774           -0.225284  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_FOLDER = \"processed_dataset\"\n",
    "sl = 128\n",
    "data_file_path = os.path.join(SAVE_FOLDER, f\"data_{sl}.pkl\")\n",
    "meta_file_path = os.path.join(SAVE_FOLDER, f\"metadata_{sl}.csv\")\n",
    "\n",
    "df = pd.read_pickle(data_file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{607853238: {'phrase': array([ 1,  5,  9,  3, 36,  3, 28, 25, 17,  3, 24, 18, 38, 32, 33, 28, 27,\\n  ...\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_df = pd.read_csv(meta_file_path, header=0)\n",
    "\n",
    "max_phrase_len = max([len(it) for it in metadata_df.phrase.values])\n",
    "possible_characters = sorted(set.union(*[set(p) for p in metadata_df.phrase.values]))\n",
    "token_map = {c: i+3 for i, c in enumerate(possible_characters)}\n",
    "PADDING = 'P'\n",
    "SOS = '<'\n",
    "EOS = '>'\n",
    "token_map[PADDING] = 0 # padding\n",
    "token_map[SOS] = 1 # SOS\n",
    "token_map[EOS] = 2 # EOS\n",
    "metadata_df.phrase = metadata_df.phrase.apply(lambda it: np.array([token_map[c] for c in '<'+it+'>'+('P'*(max_phrase_len-len(it)))], dtype=np.int32))\n",
    "max_phrase_len_with_sequence_control_tokens = max_phrase_len + 2 # SOS, EOS\n",
    "\n",
    "# Shuffle dataset\n",
    "metadata_df = metadata_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "index = {row[0]: {\"phrase\": row[1], \"signer_id\": row[2]} for row in metadata_df.to_numpy()}\n",
    "str(index)[:100] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    # Data\n",
    "    \"d_model\": 32,\n",
    "    \"seq_len\": 256,\n",
    "    \"padding_token\": token_map[PADDING],\n",
    "\n",
    "    # Training\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 1,\n",
    "\n",
    "    # Transformer\n",
    "    \"num_nodes_per_hand\": 21,\n",
    "    \"num_features_per_node\": 3,\n",
    "    \"tgt_vocab_size\": len(token_map),\n",
    "    \"num_heads\": 2,\n",
    "    \"num_enc_layer\": 2,\n",
    "    \"num_dec_layer\": 4,\n",
    "    \"dff\": 32,\n",
    "    \"dropout\": 0.1,\n",
    "    \"phrase_length\": max_phrase_len_with_sequence_control_tokens,\n",
    "\n",
    "    # LandMarkEmbedding\n",
    "    \"num_conv_layers\": 3,\n",
    "    \"filter_size\": 11,\n",
    "\n",
    "    # GraphEmbedding\n",
    "    \"hidden_dim\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, meta_data, seq_len=256, padding_value=0.0):\n",
    "        self.df = df\n",
    "        self.meta_data = meta_data\n",
    "        self.sequence_ids = df.index.unique()\n",
    "        self.padding_value = padding_value\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequence_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence_id = self.sequence_ids[idx]\n",
    "        x_values = torch.tensor(self.df.loc[sequence_id].values, dtype=torch.float32)\n",
    "\n",
    "        x_mask = torch.ones(self.seq_len, dtype=torch.float32)\n",
    "\n",
    "        # Apply padding if the sequence is shorter than seq_len\n",
    "        if x_values.shape[0] < self.seq_len:\n",
    "            x_mask[x_values.shape[0]:] = settings[\"padding_token\"]\n",
    "            padding_size = self.seq_len - x_values.shape[0]\n",
    "            padding = torch.full((padding_size, x_values.shape[1]), self.padding_value)\n",
    "            x_values = torch.cat([x_values, padding], dim=0)\n",
    "        elif x_values.shape[0] > self.seq_len:\n",
    "            # Truncate the sequence if it's longer than seq_len\n",
    "            x_values = x_values[:self.seq_len]\n",
    "\n",
    "        y_phrase = self.meta_data[sequence_id]['phrase']\n",
    "        return x_values, x_mask, y_phrase\n",
    "\n",
    "dataset = TransformerDataset(df, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4088,  0.5199,  0.6122,  ..., -0.2575, -0.2758, -0.2669],\n",
       "         [ 0.3987,  0.5237,  0.6388,  ..., -0.3157, -0.3261, -0.3143],\n",
       "         [ 0.4193,  0.5097,  0.5932,  ..., -0.2863, -0.3162, -0.3227],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.]),\n",
       " array([ 1,  7,  3, 16, 31, 18, 18, 24, 21, 28, 34, 32, 18,  2,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x:\n",
      "torch.Size([32, 256, 63])\n",
      "\n",
      "Mask for input:\n",
      "torch.Size([32, 256])\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n",
      "\n",
      "Target:\n",
      "torch.Size([32, 33])\n",
      "tensor([[ 1,  9, 12,  ...,  0,  0,  0],\n",
      "        [ 1, 14, 17,  ...,  0,  0,  0],\n",
      "        [ 1,  5,  5,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 1, 33, 14,  ...,  0,  0,  0],\n",
      "        [ 1, 10,  8,  ...,  0,  0,  0],\n",
      "        [ 1, 10,  5,  ...,  0,  0,  0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Split lengths for train (80%), test (10%), valid (10%)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
    "\n",
    "# NOTE Keep num_workers=0\n",
    "train_loader = DataLoader(train_dataset, batch_size=settings[\"batch_size\"], shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=settings[\"batch_size\"], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=settings[\"batch_size\"], shuffle=False)\n",
    "\n",
    "for it in train_loader:\n",
    "    print(\"Shape of x:\")\n",
    "    print(it[0].shape)\n",
    "    print(\"\\nMask for input:\")\n",
    "    print(it[1].shape)\n",
    "    print(it[1])\n",
    "    print(\"\\nTarget:\")\n",
    "    print(it[2].shape)\n",
    "    print(it[2]) # NOTE dec inp: it[1][:, :-1], target: it[1][:, 1:]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.7 ms ± 5.28 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len, n=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        assert d_model % 2 == 0, \"d_model must be even\"\n",
    "        \n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(n) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # NOTE\n",
    "        # register buffer in Pytorch ->\n",
    "        # If you have parameters in your model, which should be saved and restored in the state_dict,\n",
    "        # but not trained by the optimizer, you should register them as buffers.\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Broadcasting mechanism (automatically works for multiple batches even when shapes don't match along batch dim)\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarkEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, num_features, num_conv_layers, filter_size):\n",
    "        super(LandmarkEmbedding, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "\n",
    "        first_conv = nn.Conv1d(in_channels=num_features, out_channels=d_model, kernel_size=filter_size, padding='same')\n",
    "        rest_of_convs = [\n",
    "            nn.Conv1d(in_channels=d_model, out_channels=d_model, kernel_size=filter_size, padding='same')\n",
    "            for _ in range(num_conv_layers-1)\n",
    "        ]\n",
    "        \n",
    "        self.conv_block = nn.Sequential(\n",
    "            first_conv, *rest_of_convs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be of shape (batch_size, seq_len, num_of_features)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, num_of_features, seq_len)\n",
    "        x = self.conv_block(x)\n",
    "        # TODO experiment with scaling the output\n",
    "        # x *= math.sqrt(self.d_model)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, seq_len, d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4 ms ± 585 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "lm_embedding = LandmarkEmbedding(\n",
    "    d_model=settings[\"d_model\"],\n",
    "    num_features=settings[\"num_nodes_per_hand\"]*settings[\"num_features_per_node\"],\n",
    "    num_conv_layers=settings[\"num_conv_layers\"],\n",
    "    filter_size=settings[\"filter_size\"]\n",
    ")\n",
    "x = next(iter(train_loader))[0]\n",
    "%timeit lm_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv_block.0.weight | Size: torch.Size([32, 63, 11])\n",
      "Layer: conv_block.0.bias | Size: torch.Size([32])\n",
      "Layer: conv_block.1.weight | Size: torch.Size([32, 32, 11])\n",
      "Layer: conv_block.1.bias | Size: torch.Size([32])\n",
      "Layer: conv_block.2.weight | Size: torch.Size([32, 32, 11])\n",
      "Layer: conv_block.2.bias | Size: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for name, param in lm_embedding.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import GCNConv, GATConv, GATv2Conv\n",
    "\n",
    "class GraphEmbedding(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_nodes: int,\n",
    "            num_features_per_node: int,\n",
    "            d_model: int,\n",
    "            hidden_dim: int,\n",
    "            seq_len: int,\n",
    "            batch_size: int\n",
    "        ):\n",
    "        super(GraphEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_features_per_node = num_features_per_node\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # self.conv1 = GATConv(num_features_per_node, hidden_dim, heads=4, concat=False)\n",
    "        # self.conv2 = GATConv(hidden_dim, d_model, heads=4, concat=False)\n",
    "\n",
    "        self.conv1 = GCNConv(num_features_per_node, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, d_model)\n",
    "\n",
    "        # Precompute edge_index and batch_info for batch\n",
    "        # Based on: https://github.com/pyg-team/pytorch_geometric/blob/master/torch_geometric/loader/dataloader.py\n",
    "        connections = [\n",
    "            (0,1), (0,5), (0,17), (1,2), (2,3), (3,4),\n",
    "            (5,6), (5,9), (6,7), (7,8), (9,10), (9,13),\n",
    "            (10,11), (11,12), (13,14), (13,17), (14,15),\n",
    "            (15,16), (17,18), (18,19), (19,20)\n",
    "        ]\n",
    "        edges = []\n",
    "        for a, b in connections:\n",
    "            edges.append([a, b])\n",
    "            edges.append([b, a])  # Add the reverse connection\n",
    "        single_graph_edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "        # NOTE Edge index calculation can be done without using Batch.from_data_list(...) -> improve performance\n",
    "        # For a one time run this is good enough.\n",
    "        example_graph_features = torch.zeros(self.num_nodes, self.num_features_per_node)\n",
    "        data_list = [Data(x=example_graph_features, edge_index=single_graph_edge_index) for _ in range(self.batch_size*self.seq_len)]\n",
    "        mini_batch = Batch.from_data_list(data_list)\n",
    "\n",
    "        self.register_buffer('edge_index', mini_batch.edge_index)\n",
    "        self.register_buffer('batch_info', mini_batch.batch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x ~ (batch_size, seq_len, num_features)\n",
    "\n",
    "        x = x.reshape(-1, self.num_features_per_node) # (batch_size*seq_len*num_nodes, num_features_per_node)\n",
    "\n",
    "        # Obtain node embeddings\n",
    "        x = self.conv1(x, self.edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, self.edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        # Readout layer\n",
    "        x = global_mean_pool(x, self.batch_info)  # (batch_size*sequence_len, d_model)\n",
    "\n",
    "        x = x.reshape(self.batch_size, self.seq_len, self.d_model)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189 ms ± 9.62 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "graph_embedding = GraphEmbedding(\n",
    "    num_nodes=settings[\"num_nodes_per_hand\"],\n",
    "    num_features_per_node=settings[\"num_features_per_node\"],\n",
    "    d_model=settings[\"d_model\"],\n",
    "    hidden_dim=settings[\"hidden_dim\"],\n",
    "    seq_len=settings[\"seq_len\"],\n",
    "    batch_size=settings[\"batch_size\"]\n",
    ")\n",
    "x = next(iter(train_loader))[0]\n",
    "# NOTE\n",
    "# Running the Graph-based layers on each graph separately is more than 10x slower than running on the mini batch\n",
    "# To make it faster:\n",
    "# - run on GPU\n",
    "# - rewrite GNN layers using the fact that all graphs will have the same structure\n",
    "%timeit graph_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.bias | Size: torch.Size([128])\n",
      "Layer: conv1.lin.weight | Size: torch.Size([128, 3])\n",
      "Layer: conv2.bias | Size: torch.Size([32])\n",
      "Layer: conv2.lin.weight | Size: torch.Size([32, 128])\n"
     ]
    }
   ],
   "source": [
    "for name, param in graph_embedding.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Size: {param.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest of the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.masked_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.masked_self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_embedding: nn.Module,\n",
    "            tgt_vocab_size: int,\n",
    "            d_model: int,\n",
    "            num_heads: int,\n",
    "            num_enc_layers: int,\n",
    "            num_dec_layers: int,\n",
    "            d_ff: int,\n",
    "            max_seq_length: int,\n",
    "            dropout: float\n",
    "        ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = encoder_embedding\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_enc_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_dec_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src_mask, tgt):\n",
    "        src_mask = (src_mask != 0).unsqueeze(1).unsqueeze(2) # NOTE Here the src_mask contains ones and zeros\n",
    "        tgt_mask = (tgt != settings[\"padding_token\"]).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, x_mask, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(x_mask, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding = LandmarkEmbedding(\n",
    "    d_model=settings[\"d_model\"],\n",
    "    num_features=settings[\"num_nodes_per_hand\"]*settings[\"num_features_per_node\"],\n",
    "    num_conv_layers=settings[\"num_conv_layers\"],\n",
    "    filter_size=settings[\"filter_size\"]\n",
    ")\n",
    "# encoder_embedding = GraphEmbedding(\n",
    "#     num_nodes=settings[\"num_nodes_per_hand\"],\n",
    "#     num_features_per_node=settings[\"num_features_per_node\"],\n",
    "#     d_model=settings[\"d_model\"],\n",
    "#     hidden_dim=settings[\"hidden_dim\"],\n",
    "#     seq_len=settings[\"seq_len\"],\n",
    "#     batch_size=settings[\"batch_size\"]\n",
    "# )\n",
    "\n",
    "transformer = Transformer(\n",
    "    encoder_embedding=encoder_embedding,\n",
    "    tgt_vocab_size = settings[\"tgt_vocab_size\"],\n",
    "    d_model=settings[\"d_model\"],\n",
    "    num_heads=settings[\"num_heads\"],\n",
    "    num_enc_layers=settings[\"num_enc_layer\"],\n",
    "    num_dec_layers=settings[\"num_dec_layer\"],\n",
    "    d_ff=settings[\"dff\"],\n",
    "    max_seq_length=max(settings[\"phrase_length\"], settings[\"seq_len\"]),\n",
    "    dropout=settings[\"dropout\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): LandmarkEmbedding(\n",
       "    (conv_block): Sequential(\n",
       "      (0): Conv1d(63, 32, kernel_size=(11,), stride=(1,), padding=same)\n",
       "      (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=same)\n",
       "      (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=same)\n",
       "    )\n",
       "  )\n",
       "  (decoder_embedding): Embedding(40, 32)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-1): 2 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (W_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (W_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (W_o): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-3): 4 x DecoderLayer(\n",
       "      (masked_self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (W_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (W_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (W_o): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (W_k): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (W_v): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (W_o): Linear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (fc2): Linear(in_features=32, out_features=32, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=32, out_features=40, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/jamfromouterspace/levenshtein\n",
    "\n",
    "def levenshtein_distance(seq1, seq2):\n",
    "    len_1, len_2 = len(seq1), len(seq2)\n",
    "    dp = [[0] * (len_2 + 1) for _ in range(len_1 + 1)]\n",
    "\n",
    "    for i in range(len_1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len_2 + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len_1 + 1):\n",
    "        for j in range(1, len_2 + 1):\n",
    "            cost = 0 if seq1[i - 1] == seq2[j - 1] else 1\n",
    "            dp[i][j] = min(dp[i - 1][j] + 1,\n",
    "                           dp[i][j - 1] + 1,\n",
    "                           dp[i - 1][j - 1] + cost)\n",
    "\n",
    "    return dp[len_1][len_2]\n",
    "\n",
    "indices_to_ignore = torch.tensor([token_map[\" \"], token_map[\"<\"], token_map[\"P\"]]) # End of sequence is important\n",
    "\n",
    "def masked_levenshtein(output_tokens, target_tokens):\n",
    "    batch_size = output_tokens.size(0)\n",
    "    total_distance = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        pred_seq = output_tokens[i].tolist()\n",
    "        target_seq = target_tokens[i].tolist()\n",
    "\n",
    "        # Find first occurrence of EOS in both sequences\n",
    "        pred_eos_idx = next((idx for idx, token in enumerate(pred_seq) if token == token_map[EOS]), len(pred_seq))\n",
    "        target_eos_idx = next((idx for idx, token in enumerate(target_seq) if token == token_map[EOS]), len(target_seq))\n",
    "\n",
    "        # Trim the sequences at the first EOS index\n",
    "        pred_trimmed = pred_seq[:pred_eos_idx]\n",
    "        target_trimmed = target_seq[:target_eos_idx]\n",
    "\n",
    "        # Compute Levenshtein distance for the trimmed sequences\n",
    "        total_distance += levenshtein_distance(pred_trimmed, target_trimmed)\n",
    "\n",
    "    # NOTE not sum of distances are returned!\n",
    "    return total_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optional lr scheduling as in Attention Is All You Need\n",
    "# class LinearWarmupInverseSquarerootDecay(torch.optim.lr_scheduler.LambdaLR):\n",
    "#     def __init__(self, d_model, warmup_steps=4000, optimizer=None):\n",
    "#         self.d_model = d_model\n",
    "#         self.warmup_steps = warmup_steps\n",
    "#         super(LinearWarmupInverseSquarerootDecay, self).__init__(optimizer, self.lr_lambda)\n",
    "\n",
    "#     def lr_lambda(self, step_num):\n",
    "#         return (self.d_model ** -0.5) * min(step_num ** -0.5 if step_num != 0 else 1e20, step_num * (self.warmup_steps ** -1.5))\n",
    "    \n",
    "# steps = np.arange(0, 10000)\n",
    "# learning_rates = [scheduler.lr_lambda(step) for step in steps]\n",
    "\n",
    "# plt.figure(figsize=(6, 3))\n",
    "# plt.plot(steps, learning_rates, label='Learning Rate')\n",
    "# plt.title('Learning Rate Schedule')\n",
    "# plt.xlabel('Step Number')\n",
    "# plt.ylabel('Learning Rate')\n",
    "# plt.grid(True)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# scheduler = LinearWarmupInverseSquarerootDecay(d_model=d_model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 2.5188, Train Accuracy: 0.2377, Train Levenshtein: 12.6226 | Valid Loss: 2.4518, Valid Accuracy: 0.2567, Valid Levenshtein: 12.5116\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignoring padding\n",
    "#optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0)\n",
    "optimizer = torch.optim.Adam(transformer.parameters())\n",
    "\n",
    "def run_epoch(loader, is_train=True):\n",
    "    epoch_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    total_levenshtein_distance = 0\n",
    "\n",
    "    if is_train:\n",
    "        transformer.train()\n",
    "    else:\n",
    "        transformer.eval()\n",
    "\n",
    "    for batch in loader:\n",
    "        src_data, x_mask, tgt_data = batch\n",
    "        src_data = src_data.to(device)\n",
    "        x_mask = x_mask.to(device)\n",
    "        tgt_data = tgt_data.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Disable gradient calculation during validation\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            output = transformer(src_data, x_mask, tgt_data[:, :-1])\n",
    "            loss = criterion(output.contiguous().view(-1, settings[\"tgt_vocab_size\"]), \n",
    "                             tgt_data[:, 1:].contiguous().view(-1).long())\n",
    "\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Calculate masked accuracy\n",
    "        output_tokens = output.argmax(dim=-1)  # Shape: (batch_size, seq_len)\n",
    "        non_pad_mask = tgt_data[:, 1:] != settings[\"padding_token\"]  # Ignore padding tokens (mask for target data)\n",
    "        correct = (output_tokens == tgt_data[:, 1:]) & non_pad_mask  # Compare predictions to targets and ignore padding\n",
    "        total_correct += correct.sum().item()\n",
    "        total_tokens += non_pad_mask.sum().item()\n",
    "\n",
    "        # Calculate masked Levenshtein distance\n",
    "        total_levenshtein_distance += masked_levenshtein(output_tokens, tgt_data[:, 1:])\n",
    "\n",
    "    avg_loss = epoch_loss / len(loader)\n",
    "    accuracy = total_correct / total_tokens\n",
    "    return avg_loss, accuracy, total_levenshtein_distance/(settings[\"batch_size\"]*len(loader))\n",
    "\n",
    "training_metrics = []\n",
    "for epoch in range(settings[\"epochs\"]):\n",
    "    train_loss, train_accuracy, train_levenshtein = run_epoch(train_loader, is_train=True)\n",
    "    valid_loss, valid_accuracy, valid_levenshtein = run_epoch(valid_loader, is_train=False)\n",
    "    \n",
    "    display_text = f\"Epoch {epoch+1}\"\n",
    "    display_text += f\" | Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Train Levenshtein: {train_levenshtein:.4f}\"\n",
    "    display_text += f\" | Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.4f}, Valid Levenshtein: {valid_levenshtein:.4f}\"\n",
    "    print(display_text)\n",
    "\n",
    "    training_metrics.append([epoch+1, train_loss, train_accuracy, train_levenshtein, valid_loss, valid_accuracy, valid_levenshtein])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"settings.json\", \"w\") as settings_file:\n",
    "    settings_file.write(json.dumps(settings, indent=4))\n",
    "\n",
    "torch.save(transformer.state_dict(), 'model.pth')\n",
    "\n",
    "training_metrics_df = pd.DataFrame(\n",
    "    training_metrics,\n",
    "    columns=['epoch', 'train_loss', 'train_accuracy', 'train_levenshtein', 'valid_loss', 'valid_accuracy', 'valid_levenshtein']\n",
    ")\n",
    "training_metrics_df.to_csv(\"training_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      " Evaluation Results \n",
      "==================================\n",
      " Test Loss: 2.4384\n",
      " Test Accuracy: 25.69%\n",
      " Test Levenshtein Distance: 12.15\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy, test_levenshtein = run_epoch(test_loader, is_train=False)\n",
    "\n",
    "width = 34\n",
    "print(\"\\n\" + \"=\" * width)\n",
    "print(\" Evaluation Results \")\n",
    "print(\"=\" * width)\n",
    "print(f\" Test Loss: {test_loss:.4f}\")\n",
    "print(f\" Test Accuracy: {100*test_accuracy:.2f}%\")\n",
    "print(f\" Test Levenshtein Distance: {test_levenshtein:.2f}\")\n",
    "print(\"=\" * width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
